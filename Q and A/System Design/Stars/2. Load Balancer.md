### Load balancing
- Load balancing is a big topic. And unless interviewer encourages you to dive deep into load balancing topic, we better not deviate too much from the main question of the interview. Always try to stay focused on what really matters.
- load balancers will **help us achieve high throughput and availability**, **ensure requests are equally distributed among processing servers at an ideal expectation**, also **keeps track of the status of all the resources** while distributing requests. **If a server is not available** to take new requests or is not responding **or has elevated error rate**, LB will **stop sending traffic to such a server**.
### Pros
- Users experience faster, uninterrupted service. Users won’t have to wait for a single struggling server to finish its previous tasks
- help us achieve high throughput and availability, only forward traffic to “healthy” backend servers. To monitor the health of a backend server, “health checks” regularly attempt to connect to backend servers to ensure that servers are listening.
### How it works
- When domain name is hit, request is transferred to one of the VIPs registered in DNS for our domain name. VIP is resolved to a load balancer device, which has a knowledge of FrontEnd hosts.
1. load balancer seems like a single point of failure. What happens if load balancer device goes down?
2. load balancers have limits with regards to number of requests they can process and number of bytes they can transfer. What happens when our distributed message queue service becomes so popular that load balancer limits are reached?
- To address **high availability** concerns, load balancers utilize a **concept of primary and secondary nodes**. The primary node accepts connections and serves requests while the secondary node monitors the primary. If, for any reason, the primary node is unable to accept connections, the secondary node takes over.
- As for scalability concerns, a concept of multiple VIPs (sometimes referred as **VIP partitioning**) can be utilized.
- In DNS we assign multiple A records to the same DNS name for the service. As a result, requests are partitioned across several load balancers. And by spreading load balancers across several data centers, we improve both availability and performance.
```
Distributed-Msg-Queue.domain.com
|		/	VIP1 LoadBalancer A -	FrontEnd-Host-1/2.domain.com	- Data Center A
Client	-	VIP2 LoadBalancer B - 	FrontEnd-Host-3/4.domain.com	- Data Center B
		\	VIP3 LoadBalancer C - 	FrontEnd-Host-5/6.domain.com	- Data Center C
```
### (Weight) Round Robin
- Round Robin is a CPU scheduling algorithm where each process is assigned a fixed time slot in a cyclic way, When it reaches the end of the list, it starts over at the beginning 
- The weighted round-robin scheduling is designed to better handle servers with different processing capacities. Each server is assigned a weight (an integer value that indicates the processing capacity). Servers with higher weights receive new connections before those with less weights and servers with higher weights get more connections than those with less weights.
#### Pros
-  It is simple, easy to implement, and starvation-free as all processes get fair share of CPU.  
- One of the most commonly used technique in CPU scheduling as a core.  
- It is preemptive as processes are assigned CPU only for a fixed slice of time at most.  
#### Cons
- The disadvantage of it is more overhead of context switching, server load is not taken into consideration. If a server is
overloaded or slow, the LB will not stop sending new requests to that server
#### Details
- The process are dispatched in **FIFO manner but are given limited amount, called quantum/time-slice**,
- A process **does not complete before its CPU-time expires**, CPU will **take the next priority process**
- Example
```
RR with time quantum = 20
Process		Burst Time		Limit Burst Time
P1			53		=>		53 - 20 = 33 =>	33 - 20 = 13 => Completed
P2			17		(Completed)
P3			68		=>		68 - 20 = 48 => 48 - 20 = 28 => 28 - 20 = 8 => Completed
P4			24		=>		24 - 20 = 4 => COmpleted
0 	-> 	20 	-> 	37 	-> 	57 	-> 	77 -> 97 -> 117 -> 121 -> 134 -> 154 -> 162 
  P1(20)	P2(17) 	P3(20)	P4(20) P1(20)P3(20) P4(4) P1(13)  P3(20) P3(8)
```

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTM4MjE1NjU3MV19
-->